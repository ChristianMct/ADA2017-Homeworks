Given the semi-structured nature of our base dataset, data collection was a significant part of our work. The task was to extract clean and normalized features for each battle related page in the English Wikipedia dump file ($\sim $44 GB). Based on the assumption that the data collection would certainly be an iterative process, we split our pipeline in three steps, enabling each of them to be ran separately and avoiding larger scale computation to be required at each iteration.

\subsection{Page extraction}
The first pipeline step is the only one to be run on the cluster. It performs the page selection based on a regular expression matching of the page title, and outputs a consolidated JSON file containing all selected pages. By avoiding further processing at this step, we effectively reduced our dependency on the cluster, the output being already small enough for local processing ($\sim $123 MB, 27k pages, which was way beyond our initial estimate of 10k).

\subsection{Field extraction}
We leverage on the presence of an \textit{infobox} in most of the battle-related pages. This pipeline step looks for it in the page's source code, which is written using the Wikitext\cite{wikitext} markup language. The syntax generating the \textit{infobox} template\cite{template} provides a \textit{key}-\textit{value} set for each battle where the keys are conveniently standardized (e.g. \textit{date}, \textit{combatant1}, \textit{combatant2}, \dots).   Values, however, are free text field, usually containing Wikitext, and natural language formatted for humans. For this reason we qualified our dataset of being semi-structured. The output of this part is a file containing one JSON per battle, with its extracted \textit{key}-\textit{value} set, thus reducing the size to $\sim $14 MB. The number of pages where an \textit{infobox} could be found was 7486, which is within our initial range estimate, and still comfortably large. It appeared that many pages were in fact redirects, drafts or discussion pages.  

At this point, we were able to evaluate which fields could be turned into features by checking the number of concerned pages and the Wikitext processing feasibility. 

\subsection{Feature extraction}
The last pipeline step consists in turning \textit{key}-\textit{value} pairs from the previous step into clean and normalized features. This was clearly one of the most time consuming part of this project, as each field contained very different types of data and thus had to be considered separately. \\
\textbf{date} is a field of primary importance because we initially proposed to analyze the duration of battles. The main difficulty was to deal with many representation of date ranges, as well as dates happening before year 0, which python does not easily accommodate (We decided to ignore such dates after noticing they represent a very tiny fraction of all battles).\\
\textbf{combatant\_\textit{n}}: are the fields containing name(s) of the involved belligerents. Most of the battles have it populated for $n \in \{1, 2\}$, representing the two sides. The difficulty is twofold: first, the feature type needs to accommodate for coalitions of multiple parties. Second, their affiliation has to be captured at the right level of granularity. We find that a simple yet precise way of learning the state affiliation is to use the flag icons displayed next to the combatant name.\\
\textbf{result}'s content appears to vary a lot from battle to battle, ranging from strait naming of the winner to elaborated aftermath analysis. Consequently, we choose to focus on extracting the victory \textit{type} as it fits well in a categorical variable. In addition, we attribute the extracted result to one of the combatant based on string similarity. This give satisfactory results in most cases, enabling meaningful analysis within sufficiently large sets of battles.\\
\textbf{coordinates} fields are quite rarely populated, which caused problem when studying the spatial distribution of battle. We overcome this issue by extracting page-level geo-tagging data in the previous pipeline step.\\
\textbf{strength\_\textit{n}} contains a summary of side $n$'s strengths, usually in number of soldiers, but often including weapons and assets such as tanks and ships. In most cases, we are able to extract the soldier count or an estimate of it (in case where ranges of values are provided).\\
\textbf{casualties\_\textit{n}} is the most difficult field to extract normalized information from. Even though  it almost always contains a casualties amount breakdown between wounded, killed and captured, the semantical meaning of this breakdown appears very hard to capture. For example, many pages provide breakdowns coming from multiple conflicting sources, when others exhibit phase (or day) based casualties count. On the other hand, conjunction and disjunction used to attribute numbers to the casualty type are often difficult to make sense of. Thus, we decide to provide our analysis in terms of orders of magnitude, by summing all values into one single \textit{casualties\_\textit{n}} feature per side, which we would study on a logarithmic scale.