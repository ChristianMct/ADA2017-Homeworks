Given the semi-structured nature of our base dataset, data collection was a significant part of our work. The task was to extract clean and normalized features for each battle related page in the English Wikipedia dump file ($\sim $44 GB). Based on the assumption that the data collection would certainly be an iterative process, we split our pipeline in three steps, enabling each of them to be ran separately and avoiding larger scale computation to be required at each iteration.

\subsection{Page extraction}
The first pipeline step is the only one to be run on the cluster. It performs the page selection based on a regular expression matching of the page title, and outputs a consolidated JSON file containing all pages. By avoiding to process any further at this step, we effectively reduced our dependency on the cluster, the output being already small enough for local processing ($\sim $123 MB, 27k pages, which was way beyond our initial estimate of 10k).

\subsection{Field extraction}
We leverage on the presence of an \textit{infobox} in most of the battle-related pages. This step looks for it in the page's source code, which is written using the Wikitext\cite{wikitext} markup language. The syntax generating the \textit{infobox} template\cite{template} provides a \textit{key}-\textit{value} set for each battle where the keys are conveniently standardized (e.g. \textit{date}, \textit{combatant1}, \textit{combatant2}, \dots).   Values, however, are free text field, usually containing Wikitext, and natural language formatted for humans. For this reason we qualified our dataset of being semi-structured. The output of this part is a file containing one JSON per battle, with its extracted \textit{key}-\textit{value} set, thus reducing the size to $\sim $14 MB. The number of pages where an \textit{infobox} could be found was 7486, closer to our initial estimate, and still comfortably large. It appeared that many pages were in fact redirects, drafts or discussion pages.  

Using this representation, a preliminary analysis was performed to pinpoint potential features that could be extracted from the fields.

[TABLE OF FIELDS]