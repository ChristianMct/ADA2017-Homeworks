{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2; 10,000 battles Project\n",
    "\n",
    "## Tasks Description\n",
    "\n",
    "Milestone 2 (20%): the project repo contains a notebook with data collection and descriptive analysis, properly commented, and the notebook ends with a more structured and informed plan for what comes next.\n",
    "\n",
    "The tasks involving a large amount of data were pre-run and we simply describe their purposes and outputs while showing how to call them in comments.\n",
    "\n",
    "## Data Collection\n",
    "\n",
    "Data collection was a significant part of our work, given the nature of our original dataset. The task was to go from a 44 Gb large wikipedia dump to clean and normalized features about each battle. As it was shown during the lecture, data collection is in fact an iterative process: new needs in the analysis part may require new data to be extracted, or different transformation applied. Thus, the data collection was organized into a **pipeline** of 3 operations, in order to achieve **composability** and **reproducibility**. We know explain this pipeline as a part of this notebook, the actual code being organized as **python modules**, much more suited for data processing than notebooks.\n",
    "\n",
    "Each step of the pipeline is a **python script** in the `processing` folder. They have the following usage:\n",
    "\n",
    "```shell\n",
    "python script.py file-name-in file-name-out\n",
    "```\n",
    "\n",
    "For the sake of reproducibilty and organization, we used a naming convention for the output files including what step of the pipline was run, and the version of this dataset. Each version of the dataset is then associated with a git tag marking the state of the codebase that generated the file, to avoid confusion when coming back to the very begining of the pipeline each time we have a doubt (see README in the `datasets/` folder). We know describe each of the 3 pipeline operation.\n",
    "\n",
    "### Page extraction\n",
    "\n",
    "**Script**: `page_extraction.py`<br />\n",
    "**Envir.**: Cluster<br />\n",
    "**Input**: `hdfs:///datasets/wikipedia/enwiki-latest-pages-articles-multistream.xml` (~44 Gb)<br />\n",
    "**Output**: `battle-pages-v.json` (~123 Mb)<br />\n",
    "**Desc.**:<br />\n",
    "Pages extraction has two main goals, it selects what pages are (entierly) kept in the next step, and translate from an XML to a JSON representation for easier python processing.\n",
    "\n",
    "\n",
    "### Fields extraction\n",
    "\n",
    "**Script**: `fields_extraction.py`<br />\n",
    "**Envir.**: Local<br />\n",
    "**Output**: `battle-fields-v.json` (~14 Mb)\n",
    "\n",
    "### Features extraction\n",
    "\n",
    "**Script**: `features_extraction.py`<br />\n",
    "**Envir.**: Local<br />\n",
    "**Output**: `battle-features-v.json` (~4.1 Mb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO run command for pages extraction\n",
    "\n",
    "#run pages_extraction.py ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve 27255 pages with a title satisfying the regex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching of the wanted information on each page\n",
    "\n",
    "We have observed that wikipedia pages about battles often contain an \"infobox\" summarizing the main information of the battle: title, place, coordinates, dates, casualties, warriers, victory type, etc. An example is observable in https://en.wikipedia.org/wiki/Battle_of_Waterloo.\n",
    "Thus, our goal is to use these structured data to do our analysis and the next step in our data collection pipeline is the fetch the infoboxes of all the pages we retrieved in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run fields_extraction.py '/datasets/battle-pages-2.json' '/datasets/battle-fields-0.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 27255 pages, 7486 contain an infobox and only 17 contain two infoboxes (these numbers are obtained by using the python file of the next step but are described here for the clarity of the analysis). We consider that we can do a first analysis on the 7486 pages containing an infobox and do not include pages without an infobox or containing multiple infoboxes. These pages are often redirect pages !!!!!!!!!! CHECK OR REMOVE !!!!!!!!!! or pages containing incomplete information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of these first two steps\n",
    "\n",
    "##### Processing pipeline\n",
    "This document describes the data processing pipeline steps that goes from a wikipedia dump to actionable datasets.\n",
    "\n",
    "###### Pipeline steps\n",
    "\n",
    "\n",
    "##### Datasets\n",
    "\n",
    "The purpose of this page is to achieve strong reproducibility and clarity in dataset provenance by summarising the different pipeline steps and the dataset versions they \n",
    "produced.\n",
    "\n",
    "\n",
    "\n",
    "###### Pages Extraction\n",
    "\n",
    "Source file: ````hdfs:///datasets/wikipedia/enwiki-latest-pages-articles-multistream.xml```` (Cluster)\n",
    "\n",
    "Location: ````hdfs:///user/mouchet/datasets/```` (Cluster) and ````datasets/```` (Git, zipped)\n",
    "\n",
    "| File  | Version | Size | Number of battle pages | Comment |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| battle-pages-0.xml | 0.1 | 127 Mo | 24089 | Recursive \"revision\" schema |\n",
    "| battle-pages-1.xml  | 0.2 | 131 Mo | 27255 | Switched to flat schema, included Sieges|\n",
    "| battle-pages-2.json | 0.3 | 124 Mo | 27255 | Set output to JSON for easier python processing |\n",
    "\n",
    "###### Fields Extraction\n",
    "\n",
    "Location: ````datasets/```` (Git)\n",
    "\n",
    "| Source | File | Version | Size | Infobox count | Coords count | Comment |\n",
    "| ---- | --- | ------- | ---- | ------------- | ------------ | ------- |\n",
    "| battle-pages-2.json | battle-fields-0.json | 0.4 | 13 Mo | ? | ? | Extract infobox fields only |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive analysis\n",
    "\n",
    "Our first step is to observe which features are observable for the battles. For example, for how many battles the date or the geographic coordinates are available.\n",
    "\n",
    "### Available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "battles = json.load(open('../datasets/battle-fields-0.json'))\n",
    "print(\"Number of pages or battles \", len(battles))\n",
    "no_infoboxes = [b.get('infobox').get('error') for b in battles if b.get('infobox').get('error') == 'no infobox']\n",
    "print(\"Number of pages that do not contain an infobox \", len(no_infoboxes))\n",
    "double_infoboxes = [b.get('infobox').get('error') for b in battles if b.get('infobox').get('error') == 'more than one infobox']\n",
    "print(\"Number of pages that do not contain an infobox \", len(double_infoboxes))\n",
    "infoboxes = [b[\"infobox\"] for b in battles if not b[\"infobox\"].get(\"error\")]\n",
    "print(\"Number of pages that do contain an infobox \", len(infoboxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "battles = json.load(open(\"../datasets/battle-fields-0.json\"))\n",
    "df = pd.DataFrame([b[\"infobox\"] for b in battles if not b[\"infobox\"].get(\"error\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(6, 15))\n",
    "counts = df.count().sort_values(ascending=False)\n",
    "counts = counts[counts > 20]\n",
    "sns.barplot(x=counts, y=counts.index, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part, we consider the features of interest to be:\n",
    "- place\n",
    "- date\n",
    "- combatants (combatant1, ...)\n",
    "- result\n",
    "- commanders (commander1, ...)\n",
    "- conflict\n",
    "- partof (if the battle is part of a bigger war)\n",
    "- strength (strength1, ...) which are the number of soldiers\n",
    "- casualties (casualties1, ...)\n",
    "- coordinates \n",
    "As a first analysis, we can see that when an infobox is present (in 7486 of the cases), this one is often complete and contains information about most of the battles' features of interest with the exception of coordinates. For coordinates, we will also search the complete wikipedia page or use the place information (name of the city/region) to complete the coordinates when possible.\n",
    "After this step, we can confirm that we have almost complete information about 7486 battles but we have to see how these information can be used, retrieved and formatted for an explaratory analysis. Thus, we will now observe into details each feature and try to map them to usable data. For missing features, we will also see if we can retrieve these information in the page or using other existing features.\n",
    "\n",
    "### Features exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "battles = [json.loads(line) for line in open(\"../datasets/battle-features-0.json\")]\n",
    "df = pd.DataFrame(battles)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
