{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Task-1.-Compiling-Ebola-Data\"><span class=\"toc-item-num\">Task 1.&nbsp;&nbsp;</span>Compiling Ebola Data</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-2.-RNA-Sequences\"><span class=\"toc-item-num\">Task 2.&nbsp;&nbsp;</span>RNA Sequences</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-3.-Class-War-in-Titanic\"><span class=\"toc-item-num\">Task 3.&nbsp;&nbsp;</span>Class War in Titanic</a></div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../../ADA2017-Tutorials/02 - Intro to Pandas/Data' # Use the data folder provided in Tutorial 02 - Intro to Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Compiling Ebola Data\n",
    "\n",
    "The `DATA_FOLDER/ebola` folder contains summarized reports of Ebola cases from three countries (Guinea, Liberia and Sierra Leone) during the recent outbreak of the disease in West Africa. For each country, there are daily reports that contain various information about the outbreak in several cities in each country.\n",
    "\n",
    "Use pandas to import these data files into a single `Dataframe`.\n",
    "Using this `DataFrame`, calculate for *each country*, the *daily average per month* of *new cases* and *deaths*.\n",
    "Make sure you handle all the different expressions for *new cases* and *deaths* that are used in the reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None  # default='warn', Mutes warnings when copying a slice from a DataFrame.\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a frame per country containing all the data from the corresponding files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_to_dataframe(folder):\n",
    "    return pd.concat(map(pd.read_csv, glob.glob(DATA_FOLDER + folder + '/*.csv')))\n",
    "\n",
    "frameSL, frameLI, frameGU = list(map(folder_to_dataframe, ('/ebola/sl_data/', '/ebola/liberia_data/', '/ebola/guinea_data/')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand the data we have to work with, we display a summary for each data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frameSL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frameLI.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frameGU.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the names of columns identifying dates/Dates, Description/Variable and National/Totals are not unified accross the three dataframes so we change that to have the same identifiers. We also notice that the date format in the dataset for Liberia does not correspond to the one used in the other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frameSL.rename(columns={'date':'Date', 'variable':'Variable'}, inplace=True) #unifying column names\n",
    "frameLI['Date'] = pd.to_datetime(frameLI.Date) #unifying the date format\n",
    "frameSL['Date'] = pd.to_datetime(frameSL.Date) #unifying the date format\n",
    "frameGU['Date'] = pd.to_datetime(frameGU.Date) #unifying the date format\n",
    "frameGU.rename(columns={'Totals':'National','Description':'Variable' }, inplace=True) #unifying column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep only the columns relevant for our computation, these are the Date because we want to compute a monthly average of daily new cases and deaths, so we must be able to differentiate different days and months. We keep the Variable because we will need it to identify the rows relevant to new cases and new deaths. We keep the National columns because we are interested by values per country so we don't need to know the details per region specifically but just nation wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frameGU = frameGU[['Date', 'Variable', 'National']] #we keep only the relevant columns needed for our final computation\n",
    "frameSL = frameSL[['Date', 'Variable', 'National']] \n",
    "frameLI = frameLI[['Date', 'Variable', 'National']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now observe the Variable values to decide which one are relevant for our computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frameSL.Variable.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frameLI.Variable.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frameGU.Variable.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the Variables are very different in each data frame. Since we are interested by new cases and new deaths per day, we choose to keep only the confirmed new cases/deaths so as not to be biased by bad data. These variables are written in a more compact form in the Sierra Leone frame so we choose to keep their formulation for the other dataframes. For Sierra Leone the relevant variables are 'new_confirmed' and 'death_confirmed'. For Liberia it is 'Total death/s in confirmed cases' and 'New case/s (confirmed)'. For Guinea it is 'New cases of confirmed' and 'New deaths registered today (confirmed)'. However, we notice that the values for confirmed deaths in Liberia and Sierra Leone are cumulative so for a particular date we must subtract the value of the previous date so that we have only the new deaths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Sierra Leone, we start by keeping only the rows corresponding to the confirmed deaths and we remove the rows with missing values. We change the type of values in the column 'National' from string to integer and subtract to each value the previous one so that the resulting column does not have cumulative confirmed deaths but daily confirmed deaths. Once again, we remove the rows with missing data (the first row won't have any relevant data as we have no reference to subtract to it). Finally we create a single frame for Sierra Leone with our computed confirmed daily deaths and new confirmed daily cases that is sorted and indexed by the Date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frameSLDeaths = frameSL.loc[frameSL['Variable'].isin(['death_confirmed'])]\n",
    "frameSLDeathsNA = frameSLDeaths.dropna() #we remove rows with missing values\n",
    "frameSLDeathsNA.National = frameSLDeathsNA.National.apply(pd.to_numeric) #cast National values to integers\n",
    "frameSLDeathsNA.National=frameSLDeathsNA.National-frameSLDeathsNA.National.shift(1) #non cumulative deaths\n",
    "frameSLDeathsNA = frameSLDeathsNA.dropna() #we remove the first row with missing value\n",
    "frameSLNewNA = frameSL.loc[frameSL['Variable'].isin(['new_confirmed'])].dropna()\n",
    "frameTmp = [frameSLDeathsNA, frameSLNewNA]\n",
    "frameSL = pd.concat(frameTmp)\n",
    "frameSL.index=frameSL.Date\n",
    "frameSL = frameSL.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the exact same thing for Liberia: Compute the non cumulative deaths and create a single dataframe with the daily confirmed deaths and new cases that is sorted and indexed by dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frameLIDeaths = frameLI.loc[frameLI['Variable'].isin(['Total death/s in confirmed cases'])]\n",
    "frameLIDeathsNA = frameLIDeaths.dropna() #we remove rows with missing values\n",
    "frameLIDeathsNA.National = frameLIDeathsNA.National.apply(pd.to_numeric) #cast National values to integers\n",
    "frameLIDeathsNA.National=frameLIDeathsNA.National-frameLIDeathsNA.National.shift(1) #non cumulative deaths\n",
    "frameLIDeathsNA = frameLIDeathsNA.dropna() #we remove the first row with missing value\n",
    "frameLINewNA = frameLI.loc[frameLI['Variable'].isin(['New case/s (confirmed)'])]\n",
    "frameLINewNA=frameLINewNA.dropna()\n",
    "frameTmp = [frameLIDeathsNA, frameLINewNA]\n",
    "frameLI = pd.concat(frameTmp)\n",
    "frameLI.index=frameLI.Date\n",
    "frameLI = frameLI.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The death values for Guinea are already non cumulative, so we just create a single dataframe with the new deaths and cases confirmed like for the two previous countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frameGU = frameGU.loc[frameGU['Variable'].isin( ['New cases of confirmed', 'New deaths registered today (confirmed)'])]\n",
    "frameGU = frameGU.dropna() #we remove the missing values\n",
    "frameGU.index=frameGU.Date\n",
    "frameGU = frameGU.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a single dataframe using the dataframes per country created above. We concatenate the previous frames and make them accessible through keys that correspond to the right country. We rename the Variable values so that they are consistent throughout countries: all the new daily confirmed cases and deaths are described by 'new_confirmed' and 'deaths_confirmed' respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frameTmp = [frameGU, frameSL, frameLI]\n",
    "frame = pd.concat(frameTmp, keys=['Guinea', 'Sierra Leone', 'Liberia'])\n",
    "\n",
    "print(frame.Variable.unique())\n",
    "\n",
    "varMapping = {\n",
    "    'New cases of confirmed':                 'new_confirmed',\n",
    "    'New deaths registered today (confirmed)':'death_confirmed',\n",
    "    'New case/s (confirmed)':                 'new_confirmed',\n",
    "    'Total death/s in confirmed cases':       'death_confirmed'\n",
    "}\n",
    "\n",
    "frame.Variable = frame.Variable.apply(lambda v: varMapping.get(v, v))\n",
    "    \n",
    "print(frame.Variable.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the display of the resulting frame, we notice outlier values. Indeed some are negative and some are unreasonably high. We decide to remove the rows corresponding to such values. It does not make sense to set them to zero because our computation will be a daily average per month computed as a mean of the available values. We cast all the values in the National column to integer so that we can compare the values to the if conditions in order to keep only does within a reasonable boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frame['National'] = frame['National'].astype(int)\n",
    "frame = frame[[\"Variable\", \"National\"]].reset_index()\n",
    "frame = frame[(frame['National'] >=0) & (frame['National'] < 200)]\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the values now seem reasonable to use to perform our computation. We reformat the dates so that we can use them to group the deaths and new cases values per country per month and compute a mean over the days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame['DateMonth'] = frame['Date'].apply(lambda x: str(x.year) + \"-\" + str(x.month))\n",
    "deaths = frame[frame['Variable'] == \"death_confirmed\"].groupby([\"level_0\", \"DateMonth\"]).mean()[\"National\"]\n",
    "new_cases = frame[frame['Variable'] == \"new_confirmed\"].groupby([\"level_0\", \"DateMonth\"]).mean()[\"National\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now display the resulting frame with the daily average per month of new deaths per country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now display the resulting frame with the daily average per month of new cases per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. RNA Sequences\n",
    "\n",
    "In the `DATA_FOLDER/microbiome` subdirectory, there are 9 spreadsheets of microbiome data that was acquired from high-throughput RNA sequencing procedures, along with a 10<sup>th</sup> file that describes the content of each. \n",
    "\n",
    "Use pandas to import the first 9 spreadsheets into a single `DataFrame`.\n",
    "Then, add the metadata information from the 10<sup>th</sup> spreadsheet as columns in the combined `DataFrame`.\n",
    "Make sure that the final `DataFrame` has a unique index and all the `NaN` values have been replaced by the tag `unknown`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER:\n",
    "We first import all the spreadsheets in separate dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mbs = []\n",
    "for i in range(1,10):\n",
    "    temp = pd.read_excel(DATA_FOLDER+'/microbiome/MID'+str(i)+'.xls', sheetname='Sheet 1', header=None)\n",
    "    temp.columns = ['name', 'MID'+str(i)] # Using file name as column name for easier merging\n",
    "    mbs.append(temp)\n",
    "   \n",
    "    \n",
    "metadata = pd.read_excel(DATA_FOLDER+'/microbiome/metadata.xls', sheetname='Sheet1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we verify that the name column is a unique index for all the MID data and set this column as the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbs_ind = []\n",
    "doublons = False\n",
    "for i in range(1,10):\n",
    "    mbs_ind.append(mbs[i-1].set_index(['name']))\n",
    "    if mbs_ind[i-1].index.is_unique:\n",
    "        mbs[i-1] = mbs_ind[i-1]\n",
    "    else:\n",
    "        print(\"Table \" + i + \" contains doublons\")\n",
    "        doublons = True\n",
    "\n",
    "if not doublons:\n",
    "    print(\"Tables are indexed by the column name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can merge the MID tables using pandas' merge fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools as ft\n",
    "mbs_merged = ft.reduce(lambda left,right: pd.merge(left,right,left_index=True, right_index=True, how='outer'), mbs)\n",
    "mbs_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, we use the metadata file to have meaningfull headers for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we start be reordering the columns according to the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mbs_order = mbs_merged[metadata.BARCODE]\n",
    "mbs_order.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we split the table in two because the first column is 'EXTRACTION CONTROL' and does not have different types (tissue or stools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mbs_part1 = mbs_order.iloc[:, :1]\n",
    "mbs_part2 = mbs_order.iloc[:, 1:]\n",
    "mbs_part2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create the 2-levels index according to the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header = pd.MultiIndex.from_product([['NEC1','control1','NEC2','control2'],\n",
    "                                     ['tissue','stool']],\n",
    "                                    names=['Test','Type'])\n",
    "mbs_part2.columns = header\n",
    "\n",
    "mbs_part1.columns = pd.MultiIndex.from_product([['EXTRACTION CONTROL'],['']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge again all the columns together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mbs_nans = pd.merge(mbs_part1,mbs_part2,left_index=True, right_index=True, how='outer')\n",
    "mbs_nans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first check the required invariants on index and dtypes, because it is easier before replacing NaNs by \"unknown\". Then we perform the replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import issubdtype, number\n",
    "print('Index are unique:', mbs_nans.index.is_unique)\n",
    "print('All entries are either a number or nan:', issubdtype(mbs_nans.dtypes.all(), number))\n",
    "mbs_final = mbs_nans.fillna(\"unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbs_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Class War in Titanic\n",
    "\n",
    "Use pandas to import the data file `Data/titanic.xls`. It contains data on all the passengers that travelled on the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(filename=DATA_FOLDER+'/titanic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following questions state clearly your assumptions and discuss your findings:\n",
    "1. Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "2. Plot histograms for the *travel class*, *embarkation port*, *sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "3. Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*.\n",
    "4. For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*.\n",
    "5. Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*.\n",
    "6. Create 2 equally populated *age categories* and calculate survival proportions by *age category*, *travel class* and *sex*. Present your results in a `DataFrame` with unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(DATA_FOLDER+'/titanic.xls', sheetname='titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.pclass.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.survived.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sex.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.embarked.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.boat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.cabin.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[data.body.notnull()].body.unique()) == len(data[data.body.notnull()].body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    'pclass': 'category',\n",
    "    #'survived': 'category',\n",
    "    'sex': 'category',\n",
    "    'embarked': 'category',\n",
    "}\n",
    "\n",
    "cities = {\n",
    "    \"C\":\"Cherbourg\",\n",
    "    \"Q\":\"Queenstown\",\n",
    "    \"S\":\"Southampton\"\n",
    "}\n",
    "\n",
    "passengers = pd.read_excel(DATA_FOLDER+'/titanic.xls', sheetname='titanic', dtype=categories)\n",
    "\n",
    "passengers.embarked.cat.categories = [cities[c] for c in passengers.embarked.cat.categories]\n",
    "passengers.embarked.cat.add_categories([\"Unknown\"], inplace=True)\n",
    "passengers.embarked.fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "passengers.survived = passengers.survived.astype('category', categories=[0, 1], ordered=True)\n",
    "\n",
    "# Should inlcude unknown ages ? \n",
    "passengers[\"age_cat\"] = pd.cut(passengers.age, range(0, np.ceil(passengers.age.max()).astype('int')+1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, sharey=True, figsize=(15,15))\n",
    "sns.countplot(x=\"pclass\", data=passengers, ax=ax1);\n",
    "sns.countplot(x='embarked', data=passengers, ax=ax2);\n",
    "sns.countplot(x='sex', data=passengers, ax=ax3);\n",
    "sns.countplot(x='age_cat', data=passengers, ax=ax4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_floor(o):\n",
    "    if type(o) != str:\n",
    "        return np.NaN\n",
    "    else:\n",
    "        floor_codes = list(set([c for c in o if c.isalpha()]))\n",
    "        return floor_codes[0] if len(floor_codes) == 1 else np.NaN\n",
    "\n",
    "passengers['floor'] = passengers.cabin.transform(get_floor).astype('category')\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(5,5))\n",
    "passengers.floor.value_counts(sort=False).plot.pie(); #  We could have better precision by infering  missing value from class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=3, figsize=(15,5))\n",
    "colors = ['lightcoral','darkseagreen']\n",
    "labels = ['Died', 'Survived']\n",
    "for (g, s), ax in zip(passengers.groupby(['pclass']).survived, axs):\n",
    "    s.value_counts(sort=False).plot.pie(ax=ax, colors=colors, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "width = 0.25\n",
    "for i, (grp_lab, grp) in enumerate(passengers.groupby([\"pclass\", \"sex\"]).survived):\n",
    "    shift = (grp_lab[1]=='female')*(width+0.01)\n",
    "    down = plt.bar(grp_lab[0]+shift, grp.value_counts()[0], width, color=colors[0])\n",
    "    up = plt.bar(grp_lab[0]+shift, grp.value_counts()[1], width, bottom=grp.value_counts()[0], color=colors[1])\n",
    "\n",
    "ax.set_xticks(passengers.pclass.cat.categories+0.5*width)\n",
    "ax.set_xticklabels(passengers.pclass.cat.categories)\n",
    "\n",
    "ax.set_title('Surviva count per class (Male, Female)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xlabel('Class (Male, Female)')\n",
    "plt.legend(['Died', 'Survived'], loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 2 equally populated age categories and calculate survival proportions by age category, travel class and sex. Present your results in a DataFrame with unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(passengers.age.isnull().sum())\n",
    "print(passengers.age.mean())\n",
    "pd.qcut(passengers.age, [0, 0.5, 1]).value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
